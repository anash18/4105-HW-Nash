{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "womp womp\n",
      "time: 0 ns (started: 2023-12-15 10:53:50 -05:00)\n"
     ]
    }
   ],
   "source": [
    "print(\"womp womp\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autotime extension is already loaded. To reload it, use:\n",
      "  %reload_ext autotime\n",
      "time: 0 ns (started: 2023-12-15 10:53:50 -05:00)\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import torch.optim as optim\n",
    "from collections import OrderedDict\n",
    "from torchvision import datasets\n",
    "from torchvision import transforms\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn import metrics\n",
    "from sklearn.metrics import precision_score, recall_score, accuracy_score\n",
    "\n",
    "%load_ext autotime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>age</th>\n",
       "      <th>sex</th>\n",
       "      <th>cp</th>\n",
       "      <th>trtbps</th>\n",
       "      <th>chol</th>\n",
       "      <th>fbs</th>\n",
       "      <th>restecg</th>\n",
       "      <th>thalachh</th>\n",
       "      <th>exng</th>\n",
       "      <th>oldpeak</th>\n",
       "      <th>slp</th>\n",
       "      <th>caa</th>\n",
       "      <th>thall</th>\n",
       "      <th>output</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>63</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>145</td>\n",
       "      <td>233</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>150</td>\n",
       "      <td>0</td>\n",
       "      <td>2.3</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>37</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>130</td>\n",
       "      <td>250</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>187</td>\n",
       "      <td>0</td>\n",
       "      <td>3.5</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>41</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>130</td>\n",
       "      <td>204</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>172</td>\n",
       "      <td>0</td>\n",
       "      <td>1.4</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>56</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>120</td>\n",
       "      <td>236</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>178</td>\n",
       "      <td>0</td>\n",
       "      <td>0.8</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>57</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>120</td>\n",
       "      <td>354</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>163</td>\n",
       "      <td>1</td>\n",
       "      <td>0.6</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   age  sex  cp  trtbps  chol  fbs  restecg  thalachh  exng  oldpeak  slp  \\\n",
       "0   63    1   3     145   233    1        0       150     0      2.3    0   \n",
       "1   37    1   2     130   250    0        1       187     0      3.5    0   \n",
       "2   41    0   1     130   204    0        0       172     0      1.4    2   \n",
       "3   56    1   1     120   236    0        1       178     0      0.8    2   \n",
       "4   57    0   0     120   354    0        1       163     1      0.6    2   \n",
       "\n",
       "   caa  thall  output  \n",
       "0    0      1       1  \n",
       "1    0      2       1  \n",
       "2    0      2       1  \n",
       "3    0      2       1  \n",
       "4    0      2       1  "
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time: 16 ms (started: 2023-12-15 10:53:50 -05:00)\n"
     ]
    }
   ],
   "source": [
    "file_path = r\"C:\\Users\\aaron\\Documents\\Python Code\\Datasets\\Heart Attack Analysis and Prediction\\heart.csv\"\n",
    "\n",
    "heart_attack = pd.read_csv(file_path)\n",
    "\n",
    "heart_attack.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time: 0 ns (started: 2023-12-15 10:53:50 -05:00)\n"
     ]
    }
   ],
   "source": [
    "#Defining mapping function to map multiple inputs of the dataset\n",
    "def gender_map(x):\n",
    "    return x.map({'M': 1, 'F': 0, 'Male' : 1, \"Female\" : 0, '1' : 1, '0' : 0})\n",
    "\n",
    "def hemisphere_map(x):\n",
    "    return x.map({'Southern Hemisphere' : -1, 'Northern Hemisphere' : 1})\n",
    "\n",
    "def countries_map(x):\n",
    "    return x.map({'Argentina' : -1, 'Brazil' : -1, 'China' : -1, 'Colombia' : -1,\n",
    "    'India' : -1, 'Nigeria' : -1, 'South Africa' : -1, 'South Korea' : -1, 'Thailand' : -1,\n",
    "    'Vietnam' : -1, 'Australia' : 1, 'Canada' : 1, 'France' : 1, 'Germany' : 1,\n",
    "    'Italy' : 1, 'Japan' : 1, 'New Zealand' : 1, 'Spain' : 1, 'United Kingdom' : 1,\n",
    "    'United States' : 1})\n",
    "\n",
    "def diet_map(x):\n",
    "    return x.map({'Unhealthy' : -1, 'Average' : 0, 'Healthy' : 1})\n",
    "\n",
    "def continent_map(x):\n",
    "    return x.map({'Asia' : 0, 'Africa' : 1, 'Europe' : 2, 'North America' : 3,\n",
    "    'South America' : 4, 'Australia' : 5})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time: 16 ms (started: 2023-12-15 10:53:50 -05:00)\n"
     ]
    }
   ],
   "source": [
    "# heart_attack['Sex'] = heart_attack[['Sex']].apply(gender_map)\n",
    "# heart_attack['Hemisphere'] = heart_attack[['Hemisphere']].apply(hemisphere_map)\n",
    "# heart_attack['Country'] = heart_attack[['Country']].apply(countries_map)\n",
    "# heart_attack['Diet'] = heart_attack[['Diet']].apply(diet_map)\n",
    "# heart_attack['Continent'] = heart_attack[['Continent']].apply(continent_map)\n",
    "\n",
    "# unique_sex = heart_attack['Sex'].unique()\n",
    "# print(unique_sex)\n",
    "# unique = heart_attack['Hemisphere'].unique()\n",
    "# print(unique)\n",
    "# unique_country = heart_attack['Country'].unique()\n",
    "# print(unique_country)\n",
    "# unique_diet = heart_attack['Diet'].unique()\n",
    "# print(unique_diet)\n",
    "# unique_continent = heart_attack['Continent'].unique()\n",
    "# print(unique_continent)\n",
    "\n",
    "# heart_attack.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Columns with NaN values after fillna operations:\n",
      "[]\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>age</th>\n",
       "      <th>sex</th>\n",
       "      <th>cp</th>\n",
       "      <th>trtbps</th>\n",
       "      <th>chol</th>\n",
       "      <th>fbs</th>\n",
       "      <th>restecg</th>\n",
       "      <th>thalachh</th>\n",
       "      <th>exng</th>\n",
       "      <th>oldpeak</th>\n",
       "      <th>slp</th>\n",
       "      <th>caa</th>\n",
       "      <th>thall</th>\n",
       "      <th>output</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>63</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>145</td>\n",
       "      <td>233</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>150</td>\n",
       "      <td>0</td>\n",
       "      <td>2.3</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>37</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>130</td>\n",
       "      <td>250</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>187</td>\n",
       "      <td>0</td>\n",
       "      <td>3.5</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>41</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>130</td>\n",
       "      <td>204</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>172</td>\n",
       "      <td>0</td>\n",
       "      <td>1.4</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>56</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>120</td>\n",
       "      <td>236</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>178</td>\n",
       "      <td>0</td>\n",
       "      <td>0.8</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>57</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>120</td>\n",
       "      <td>354</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>163</td>\n",
       "      <td>1</td>\n",
       "      <td>0.6</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   age  sex  cp  trtbps  chol  fbs  restecg  thalachh  exng  oldpeak  slp  \\\n",
       "0   63    1   3     145   233    1        0       150     0      2.3    0   \n",
       "1   37    1   2     130   250    0        1       187     0      3.5    0   \n",
       "2   41    0   1     130   204    0        0       172     0      1.4    2   \n",
       "3   56    1   1     120   236    0        1       178     0      0.8    2   \n",
       "4   57    0   0     120   354    0        1       163     1      0.6    2   \n",
       "\n",
       "   caa  thall  output  \n",
       "0    0      1       1  \n",
       "1    0      2       1  \n",
       "2    0      2       1  \n",
       "3    0      2       1  \n",
       "4    0      2       1  "
      ]
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time: 0 ns (started: 2023-12-15 10:53:50 -05:00)\n"
     ]
    }
   ],
   "source": [
    "# mode_columns = ['Stress Level','Medication Use', 'Previous Heart Problems', 'Diet', 'Physical Activity Days Per Week', 'Alcohol Consumption', 'Obesity', 'Smoking', \n",
    "#                 'Family History', 'Diabetes', 'Hemisphere', 'Continent', 'Country']\n",
    "\n",
    "# mean_columns = ['Exercise Hours Per Week', 'Sleep Hours Per Day', 'BMI', 'Sedentary Hours Per Day']\n",
    "\n",
    "# median_columns = ['Heart Rate', 'Income', 'Triglycerides']\n",
    "\n",
    "# # Fill NaN values with the mode of the column for mode_columns\n",
    "# for col in mode_columns:\n",
    "#     heart_attack[col].fillna(heart_attack[col].mode()[0], inplace=True)\n",
    "\n",
    "# # Fill NaN values with the median of the column for median_columns\n",
    "# for col in median_columns:\n",
    "#     heart_attack[col].fillna(heart_attack[col].median(), inplace=True)\n",
    "\n",
    "# # Fill NaN values with the mean of the column for mean_columns\n",
    "# for col in mean_columns:\n",
    "#     heart_attack[col].fillna(heart_attack[col].mean(), inplace=True)\n",
    "\n",
    "\n",
    "# unique_sex = heart_attack['Sex'].unique()\n",
    "# print(unique_sex)\n",
    "# unique = heart_attack['Hemisphere'].unique()\n",
    "# print(unique)\n",
    "# unique_country = heart_attack['Country'].unique()\n",
    "# print(unique_country)\n",
    "# unique_diet = heart_attack['Diet'].unique()\n",
    "# print(unique_diet)\n",
    "# unique_continent = heart_attack['Continent'].unique()\n",
    "# print(unique_continent)\n",
    "\n",
    "\n",
    "# Check for NaN values in columns after fillna operations\n",
    "columns_with_nulls = heart_attack.columns[heart_attack.isnull().any()].tolist()\n",
    "print(\"Columns with NaN values after fillna operations:\")\n",
    "print(columns_with_nulls)\n",
    "\n",
    "heart_attack.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GPU is not available, using CPU.\n",
      "time: 0 ns (started: 2023-12-15 10:53:50 -05:00)\n"
     ]
    }
   ],
   "source": [
    "# Check if GPU is available\n",
    "if torch.cuda.is_available():\n",
    "    device = torch.device(\"cuda\")  # Use GPU\n",
    "    print(\"GPU is available.\")\n",
    "else:\n",
    "    device = torch.device(\"cpu\")   # Use CPU\n",
    "    print(\"GPU is not available, using CPU.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.float32\n",
      "torch.float32\n",
      "time: 0 ns (started: 2023-12-15 10:53:50 -05:00)\n"
     ]
    }
   ],
   "source": [
    "#Splitting the Variables into X and Y\n",
    "X = heart_attack.iloc[:, 0:12].values\n",
    "Y = heart_attack.iloc[:, 13].values\n",
    "\n",
    "# Split our Data set into Training Data and val Data.\n",
    "X_train, X_val, Y_train, Y_val = train_test_split(X, Y, test_size = 0.2, random_state = 0)\n",
    "\n",
    "Y_train = Y_train.reshape(-1, 1)\n",
    "Y_val = Y_val.reshape(-1, 1)\n",
    "\n",
    "# Convert the numpy arrays into tensors\n",
    "Y_train_t = torch.tensor(Y_train, dtype=torch.float)\n",
    "Y_val_t = torch.tensor(Y_val, dtype=torch.float)\n",
    "\n",
    "\n",
    "print(Y_train_t.dtype)\n",
    "print(Y_val_t.dtype)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time: 0 ns (started: 2023-12-15 10:53:50 -05:00)\n"
     ]
    }
   ],
   "source": [
    "# Defining PCA function\n",
    "def PCA_function(X_train, X_val, K):\n",
    "  pca = PCA(n_components=K)\n",
    "  X_train_PCA = pca.fit_transform(X_train)\n",
    "  X_val_PCA = pca.fit_transform(X_val)\n",
    "  return X_train_PCA, X_val_PCA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The shape of X_train after PCA is (242, 2)\n",
      "The shape of X_val after PCA is (61, 2)\n",
      "\n",
      "The shape of the X_train tensor is torch.Size([242, 2])\n",
      "The shape of the X_val tensor is torch.Size([61, 2])\n",
      "time: 0 ns (started: 2023-12-15 10:53:50 -05:00)\n"
     ]
    }
   ],
   "source": [
    "(X_train_PCA, X_val_PCA) = PCA_function(X_train, X_val, 2)\n",
    "\n",
    "print(\"The shape of X_train after PCA is\", X_train_PCA.shape)\n",
    "print(\"The shape of X_val after PCA is\", X_val_PCA.shape)\n",
    "\n",
    "# Use standard scaling from Sklearn to scale data between - and  for better accuracy.\n",
    "scale = StandardScaler()\n",
    "X_train_PCA = scale.fit_transform(X_train_PCA)\n",
    "X_val_PCA = scale.fit_transform(X_val_PCA)\n",
    "\n",
    "X_train_PCA_t = torch.tensor(X_train_PCA, dtype=torch.float)\n",
    "X_val_PCA_t = torch.tensor(X_val_PCA, dtype=torch.float)\n",
    "\n",
    "print(\"\\nThe shape of the X_train tensor is\", X_train_PCA_t.shape)\n",
    "print(\"The shape of the X_val tensor is\", X_val_PCA_t.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time: 0 ns (started: 2023-12-15 10:53:50 -05:00)\n"
     ]
    }
   ],
   "source": [
    "def model_linear(t_in, w1, w2, w3, b):\n",
    "  t_p = ((w3 * t_in ** 3) + (w2 * t_in ** 2) + (w1 * t_in) + b)\n",
    "  return t_p\n",
    "\n",
    "def loss_fn(t_p, t_gnd):\n",
    "  squared_diffs = (t_p - t_gnd)**2\n",
    "  return squared_diffs.mean()\n",
    "\n",
    "def training_loop(n_epochs, optimizer, params, train_t_in, val_t_in,\n",
    "                    train_t_out, val_t_out):\n",
    "  for epoch in range(1, n_epochs + 1):\n",
    "\n",
    "    if params.grad is not None:\n",
    "      params.grad.zero_()\n",
    "\n",
    "    train_t_p = model_linear(train_t_in, *params)\n",
    "    train_loss = loss_fn(train_t_p, train_t_out)\n",
    "\n",
    "    val_t_p = model_linear(val_t_in, *params)\n",
    "    val_loss = loss_fn(val_t_p, val_t_out)\n",
    "\n",
    "    optimizer.zero_grad()\n",
    "    train_loss.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "    if epoch <= 3 or epoch % 500 == 0:\n",
    "      print(f\"Epoch {epoch}, \\tTraining loss {train_loss.item():.4f},\"\n",
    "      f\" \\tValidation loss {val_loss.item():.4f}\")\n",
    "\n",
    "  return params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss every 500 epochs using learning rate of 1e-3\n",
      "\n",
      "Epoch 1, \tTraining loss 156.1649, \tValidation loss 55.7484\n",
      "Epoch 2, \tTraining loss 97.7000, \tValidation loss 36.7891\n",
      "Epoch 3, \tTraining loss 61.3619, \tValidation loss 24.7086\n",
      "Epoch 500, \tTraining loss 0.5914, \tValidation loss 0.6744\n",
      "Epoch 1000, \tTraining loss 0.3391, \tValidation loss 0.3766\n",
      "Epoch 1500, \tTraining loss 0.2652, \tValidation loss 0.2809\n",
      "Epoch 2000, \tTraining loss 0.2420, \tValidation loss 0.2461\n",
      "Epoch 2500, \tTraining loss 0.2342, \tValidation loss 0.2322\n",
      "Epoch 3000, \tTraining loss 0.2314, \tValidation loss 0.2261\n",
      "Epoch 3500, \tTraining loss 0.2304, \tValidation loss 0.2233\n",
      "Epoch 4000, \tTraining loss 0.2301, \tValidation loss 0.2219\n",
      "Epoch 4500, \tTraining loss 0.2299, \tValidation loss 0.2212\n",
      "Epoch 5000, \tTraining loss 0.2299, \tValidation loss 0.2208\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([-0.1566,  0.0099,  0.0056,  0.5233], requires_grad=True)"
      ]
     },
     "execution_count": 78,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time: 1.44 s (started: 2023-12-15 10:53:50 -05:00)\n"
     ]
    }
   ],
   "source": [
    "params = torch.tensor([1.0, 1.0, 1.0, 0.0], requires_grad=True)\n",
    "\n",
    "learning_rate_1 = 1e-3\n",
    "optimizer_1 = optim.SGD([params], lr=learning_rate_1)\n",
    "\n",
    "print('Loss every 500 epochs using learning rate of 1e-3\\n')\n",
    "training_loop(\n",
    "    n_epochs = 5000,\n",
    "    optimizer = optimizer_1,\n",
    "    params = params,\n",
    "    train_t_in = X_train_PCA_t,\n",
    "    val_t_in = X_val_PCA_t,\n",
    "    train_t_out = Y_train_t,\n",
    "    val_t_out = Y_val_t)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Accuracy: 0.6157\n",
      "Training Precision: 0.6142\n",
      "Training Recall: 0.6148\n",
      "Training F1 Score: 0.6145\n",
      "\n",
      "Validation Accuracy: 0.5410\n",
      "Validation Precision: 0.5460\n",
      "Validation Recall: 0.5463\n",
      "Validation F1 Score: 0.5461\n",
      "time: 16 ms (started: 2023-12-15 10:53:52 -05:00)\n"
     ]
    }
   ],
   "source": [
    "# Getting the predictions based on the training set using the linear model\n",
    "model_output = model_linear(X_train_PCA_t, *params)\n",
    "# Converting the prediction tensor into a numpy array to use the sklearn metrics library\n",
    "model_output_np = model_output.detach().numpy() if isinstance(model_output, torch.Tensor) else model_output\n",
    "\n",
    "# Using argmax to get the class (1 or 0) with the highest probability\n",
    "predicted_labels = np.argmax(model_output_np, axis=1)\n",
    "\n",
    "# Calculate accuracy, precision, and recall for training set\n",
    "train_accuracy = accuracy_score(Y_train, predicted_labels)\n",
    "train_recall = recall_score(Y_train, predicted_labels, average= 'macro', zero_division=1)\n",
    "train_precision = precision_score(Y_train, predicted_labels, average = 'macro', zero_division=1)\n",
    "train_F1_score = (2*train_recall*train_precision)/(train_recall+train_precision)\n",
    "\n",
    "# Getting the predictions based on the validation set using the linear model\n",
    "model_output_val = model_linear(X_val_PCA_t, *params)\n",
    "# Converting the prediction tensor into a numpy array to use the sklearn metrics library\n",
    "model_output_val_np = model_output_val.detach().numpy() if isinstance(model_output_val, torch.Tensor) else model_output_val\n",
    "\n",
    "# Using argmax to get the class (1 or 0) with the highest probability\n",
    "predicted_labels_val = np.argmax(model_output_val_np, axis=1)\n",
    "\n",
    "# Calculate accuracy, precision, and recall for validation set\n",
    "val_accuracy = accuracy_score(Y_val, predicted_labels_val)\n",
    "val_precision = precision_score(Y_val, predicted_labels_val, average = 'macro', zero_division=1)\n",
    "val_recall = recall_score(Y_val, predicted_labels_val, average = 'macro', zero_division=1)\n",
    "val_F1_score = (2*val_recall*val_precision)/(val_recall+val_precision)\n",
    "\n",
    "# Print the calculated metrics\n",
    "print(f\"Training Accuracy: {train_accuracy:.4f}\")\n",
    "print(f\"Training Precision: {train_precision:.4f}\")\n",
    "print(f\"Training Recall: {train_recall:.4f}\")\n",
    "print(f\"Training F1 Score: {train_F1_score:.4f}\")\n",
    "\n",
    "print(f\"\\nValidation Accuracy: {val_accuracy:.4f}\")\n",
    "print(f\"Validation Precision: {val_precision:.4f}\")\n",
    "print(f\"Validation Recall: {val_recall:.4f}\")\n",
    "print(f\"Validation F1 Score: {val_F1_score:.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Sequential(\n",
       "  (0): Linear(in_features=2, out_features=1024, bias=True)\n",
       "  (1): Tanh()\n",
       "  (2): Linear(in_features=1024, out_features=2048, bias=True)\n",
       "  (3): Tanh()\n",
       "  (4): Linear(in_features=2048, out_features=512, bias=True)\n",
       "  (5): Tanh()\n",
       "  (6): Linear(in_features=512, out_features=1, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 80,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time: 16 ms (started: 2023-12-15 10:53:52 -05:00)\n"
     ]
    }
   ],
   "source": [
    "#Sequential Model\n",
    "\n",
    "model_seq = nn.Sequential(\n",
    "    nn.Linear(2, 1024),\n",
    "    nn.Tanh(),\n",
    "    nn.Linear(1024, 2048),\n",
    "    nn.Tanh(),\n",
    "    nn.Linear(2048, 512),\n",
    "    nn.Tanh(),\n",
    "    nn.Linear(512, 1)\n",
    "            )\n",
    "model_seq"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3151873"
      ]
     },
     "execution_count": 81,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time: 0 ns (started: 2023-12-15 10:53:52 -05:00)\n"
     ]
    }
   ],
   "source": [
    "sum([p.numel() for p in model_seq.parameters()])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time: 0 ns (started: 2023-12-15 10:53:52 -05:00)\n"
     ]
    }
   ],
   "source": [
    "#Let's define our training loop that will be used\n",
    "\n",
    "def training_loop_seq(n_epochs, optimizer, model, loss_fn, x_train, x_val,\n",
    "                  y_train, y_val):\n",
    "    for epoch in range(1, n_epochs + 1):\n",
    "        p_train = model(x_train) # <1>\n",
    "        loss_train = loss_fn(p_train, y_train,)\n",
    "\n",
    "        p_val = model(x_val) # <1>\n",
    "        loss_val = loss_fn(p_val, y_val)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        loss_train.backward() # <2>\n",
    "        optimizer.step()\n",
    "\n",
    "\n",
    "        if epoch == 1 or epoch % 10 == 0:\n",
    "            print(f\"Epoch: {epoch}, Training loss: {loss_train.item():.4f},\"\n",
    "                  f\" Validation loss: {loss_val.item():.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1, Training loss: 0.6060, Validation loss: 0.6218\n",
      "Epoch: 10, Training loss: 0.2702, Validation loss: 0.2649\n",
      "Epoch: 20, Training loss: 0.2063, Validation loss: 0.1951\n",
      "Epoch: 30, Training loss: 0.2085, Validation loss: 0.1931\n",
      "Epoch: 40, Training loss: 0.1986, Validation loss: 0.1927\n",
      "Epoch: 50, Training loss: 0.1991, Validation loss: 0.1971\n",
      "Epoch: 60, Training loss: 0.1983, Validation loss: 0.1956\n",
      "Epoch: 70, Training loss: 0.1978, Validation loss: 0.1937\n",
      "Epoch: 80, Training loss: 0.1976, Validation loss: 0.1925\n",
      "Epoch: 90, Training loss: 0.1974, Validation loss: 0.1920\n",
      "Epoch: 100, Training loss: 0.1973, Validation loss: 0.1919\n",
      "Epoch: 110, Training loss: 0.1972, Validation loss: 0.1920\n",
      "Epoch: 120, Training loss: 0.1971, Validation loss: 0.1921\n",
      "Epoch: 130, Training loss: 0.1970, Validation loss: 0.1921\n",
      "Epoch: 140, Training loss: 0.1969, Validation loss: 0.1921\n",
      "Epoch: 150, Training loss: 0.1967, Validation loss: 0.1921\n",
      "Epoch: 160, Training loss: 0.1966, Validation loss: 0.1922\n",
      "Epoch: 170, Training loss: 0.1965, Validation loss: 0.1924\n",
      "Epoch: 180, Training loss: 0.1964, Validation loss: 0.1926\n",
      "Epoch: 190, Training loss: 0.1962, Validation loss: 0.1928\n",
      "Epoch: 200, Training loss: 0.1961, Validation loss: 0.1930\n",
      "Epoch: 210, Training loss: 0.1959, Validation loss: 0.1933\n",
      "Epoch: 220, Training loss: 0.1957, Validation loss: 0.1937\n",
      "Epoch: 230, Training loss: 0.1955, Validation loss: 0.1941\n",
      "Epoch: 240, Training loss: 0.1952, Validation loss: 0.1947\n",
      "Epoch: 250, Training loss: 0.1948, Validation loss: 0.1954\n",
      "Epoch: 260, Training loss: 0.1943, Validation loss: 0.1961\n",
      "Epoch: 270, Training loss: 0.1936, Validation loss: 0.1968\n",
      "Epoch: 280, Training loss: 0.1928, Validation loss: 0.1974\n",
      "Epoch: 290, Training loss: 0.1919, Validation loss: 0.1978\n",
      "Epoch: 300, Training loss: 0.1909, Validation loss: 0.1980\n",
      "Epoch: 310, Training loss: 0.1900, Validation loss: 0.1982\n",
      "Epoch: 320, Training loss: 0.1891, Validation loss: 0.1984\n",
      "Epoch: 330, Training loss: 0.1883, Validation loss: 0.1987\n",
      "Epoch: 340, Training loss: 0.1875, Validation loss: 0.1988\n",
      "Epoch: 350, Training loss: 0.1868, Validation loss: 0.1991\n",
      "Epoch: 360, Training loss: 0.1860, Validation loss: 0.1994\n",
      "Epoch: 370, Training loss: 0.1855, Validation loss: 0.2008\n",
      "Epoch: 380, Training loss: 0.1849, Validation loss: 0.1985\n",
      "Epoch: 390, Training loss: 0.1840, Validation loss: 0.2006\n",
      "Epoch: 400, Training loss: 0.1833, Validation loss: 0.2017\n",
      "Epoch: 410, Training loss: 0.1825, Validation loss: 0.2010\n",
      "Epoch: 420, Training loss: 0.1817, Validation loss: 0.2010\n",
      "Epoch: 430, Training loss: 0.1808, Validation loss: 0.2010\n",
      "Epoch: 440, Training loss: 0.1799, Validation loss: 0.2009\n",
      "Epoch: 450, Training loss: 0.1789, Validation loss: 0.2008\n",
      "Epoch: 460, Training loss: 0.1783, Validation loss: 0.1985\n",
      "Epoch: 470, Training loss: 0.1773, Validation loss: 0.2046\n",
      "Epoch: 480, Training loss: 0.1760, Validation loss: 0.2001\n",
      "Epoch: 490, Training loss: 0.1751, Validation loss: 0.1990\n",
      "Epoch: 500, Training loss: 0.1741, Validation loss: 0.1995\n",
      "time: 14.8 s (started: 2023-12-15 10:53:52 -05:00)\n"
     ]
    }
   ],
   "source": [
    "#Let's try doing some training\n",
    "\n",
    "optimizer_1 = optim.Adam(model_seq.parameters(), lr=1e-4)\n",
    "\n",
    "training_loop_seq(\n",
    "    n_epochs = 501,\n",
    "    optimizer = optimizer_1,\n",
    "    model = model_seq,\n",
    "    loss_fn = nn.MSELoss(),\n",
    "    x_train = X_train_PCA_t,\n",
    "    x_val = X_val_PCA_t,\n",
    "    y_train = Y_train_t,\n",
    "    y_val = Y_val_t\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time: 0 ns (started: 2023-12-15 10:54:07 -05:00)\n"
     ]
    }
   ],
   "source": [
    "# Function to calculate accuracy\n",
    "def calculate_accuracy(predictions, targets):\n",
    "    return (predictions == targets).float().mean().item()\n",
    "\n",
    "# Function to calculate recall\n",
    "def calculate_recall(predictions, targets):\n",
    "    true_positives = ((predictions == 1) & (targets == 1)).sum().item()\n",
    "    false_negatives = ((predictions == 0) & (targets == 1)).sum().item()\n",
    "    return true_positives / (true_positives + false_negatives) if (true_positives + false_negatives) > 0 else 0\n",
    "\n",
    "# Function to calculate precision\n",
    "def calculate_precision(predictions, targets):\n",
    "    true_positives = ((predictions == 1) & (targets == 1)).sum().item()\n",
    "    false_positives = ((predictions == 1) & (targets == 0)).sum().item()\n",
    "    return true_positives / (true_positives + false_positives) if (true_positives + false_positives) > 0 else 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Accuracy: 0.5620\n",
      "Training Recall: 1.0000\n",
      "Training Precision: 0.5527\n",
      "Training F1 Score: 0.7120\n",
      "\n",
      "Validation Accuracy: 0.5902\n",
      "Validation Recall: 1.0000\n",
      "Validation Precision: 0.5763\n",
      "Validation F1 Score: 0.7312\n",
      "time: 15 ms (started: 2023-12-15 10:54:07 -05:00)\n"
     ]
    }
   ],
   "source": [
    "# Calculate predictions for the training set\n",
    "with torch.no_grad():\n",
    "    train_predictions = torch.round(torch.sigmoid(model_seq(X_train_PCA_t)))\n",
    "\n",
    "# Calculate accuracy, recall, precision, and F1 Score for the training set\n",
    "accuracy_train = calculate_accuracy(train_predictions, Y_train_t)\n",
    "recall_train = calculate_recall(train_predictions, Y_train_t)\n",
    "precision_train = calculate_precision(train_predictions, Y_train_t)\n",
    "F1_score_train = (2*recall_train*precision_train)/(recall_train+precision_train)\n",
    "\n",
    "# Calculate predictions for the validation set\n",
    "with torch.no_grad():\n",
    "    train_predictions_val = torch.round(torch.sigmoid(model_seq(X_val_PCA_t)))\n",
    "\n",
    "# Calculate accuracy, recall, precision, and F1 Score for the validation set\n",
    "accuracy_val = calculate_accuracy(train_predictions_val, Y_val_t)\n",
    "recall_val = calculate_recall(train_predictions_val, Y_val_t)\n",
    "precision_val = calculate_precision(train_predictions_val, Y_val_t)\n",
    "F1_score_val = (2*recall_val*precision_val)/(recall_val+precision_val)\n",
    "\n",
    "# Print the calculated metrics\n",
    "print(\"Training Accuracy: {:.4f}\".format(accuracy_train))\n",
    "print(\"Training Recall: {:.4f}\".format(recall_train))\n",
    "print(\"Training Precision: {:.4f}\".format(precision_train))\n",
    "print(\"Training F1 Score: {:.4f}\".format(F1_score_train))\n",
    "\n",
    "print(\"\\nValidation Accuracy: {:.4f}\".format(accuracy_val))\n",
    "print(\"Validation Recall: {:.4f}\".format(recall_val))\n",
    "print(\"Validation Precision: {:.4f}\".format(precision_val))\n",
    "print(\"Validation F1 Score: {:.4f}\".format(F1_score_val))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time: 15 ms (started: 2023-12-15 10:54:07 -05:00)\n"
     ]
    }
   ],
   "source": [
    "model_fnn = nn.Sequential(\n",
    "            nn.Linear(2, 1024),\n",
    "            nn.Tanh(),\n",
    "            nn.Dropout(p=0.5),  # Dropout layer with a dropout probability of 0.5\n",
    "            nn.Linear(1024, 2048),\n",
    "            nn.Tanh(),\n",
    "            nn.Dropout(p=0.5),  # Dropout layer with a dropout probability of 0.5\n",
    "            nn.Linear(2048,1024),\n",
    "            nn.Tanh(),\n",
    "            nn.Dropout(p=0.5),  # Dropout layer with a dropout probability of 0.5\n",
    "            nn.Linear(1024, 512),\n",
    "            nn.Tanh(),\n",
    "            nn.Dropout(p=0.5),  # Dropout layer with a dropout probability of 0.5\n",
    "            nn.Linear(512, 64),\n",
    "            nn.Tanh(),\n",
    "            nn.Linear(64, 1)\n",
    "           )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4758145"
      ]
     },
     "execution_count": 87,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time: 0 ns (started: 2023-12-15 10:54:07 -05:00)\n"
     ]
    }
   ],
   "source": [
    "sum([p.numel() for p in model_fnn.parameters()])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0, Loss: 0.694688\n",
      "Epoch: 10, Loss: 0.609694\n",
      "Epoch: 20, Loss: 0.598087\n",
      "Epoch: 30, Loss: 0.596970\n",
      "Epoch: 40, Loss: 0.593922\n",
      "Epoch: 50, Loss: 0.600851\n",
      "Epoch: 60, Loss: 0.591013\n",
      "Epoch: 70, Loss: 0.593637\n",
      "Epoch: 80, Loss: 0.589592\n",
      "Epoch: 90, Loss: 0.593503\n",
      "Epoch: 100, Loss: 0.599299\n",
      "Epoch: 110, Loss: 0.590558\n",
      "Epoch: 120, Loss: 0.580673\n",
      "Epoch: 130, Loss: 0.599433\n",
      "Epoch: 140, Loss: 0.585158\n",
      "Epoch: 150, Loss: 0.586089\n",
      "Epoch: 160, Loss: 0.593090\n",
      "Epoch: 170, Loss: 0.586894\n",
      "Epoch: 180, Loss: 0.581984\n",
      "Epoch: 190, Loss: 0.589195\n",
      "Epoch: 200, Loss: 0.575885\n",
      "Epoch: 210, Loss: 0.583310\n",
      "Epoch: 220, Loss: 0.585391\n",
      "Epoch: 230, Loss: 0.568722\n",
      "Epoch: 240, Loss: 0.588484\n",
      "Epoch: 250, Loss: 0.567937\n",
      "Epoch: 260, Loss: 0.600072\n",
      "Epoch: 270, Loss: 0.579487\n",
      "Epoch: 280, Loss: 0.574528\n",
      "Epoch: 290, Loss: 0.578828\n",
      "Epoch: 300, Loss: 0.564065\n",
      "Epoch: 310, Loss: 0.561638\n",
      "Epoch: 320, Loss: 0.563761\n",
      "Epoch: 330, Loss: 0.559997\n",
      "Epoch: 340, Loss: 0.556079\n",
      "Epoch: 350, Loss: 0.565035\n",
      "Epoch: 360, Loss: 0.561807\n",
      "Epoch: 370, Loss: 0.562088\n",
      "Epoch: 380, Loss: 0.563861\n",
      "Epoch: 390, Loss: 0.552213\n",
      "Epoch: 400, Loss: 0.555242\n",
      "Epoch: 410, Loss: 0.570329\n",
      "Epoch: 420, Loss: 0.566555\n",
      "Epoch: 430, Loss: 0.566691\n",
      "Epoch: 440, Loss: 0.551310\n",
      "Epoch: 450, Loss: 0.547553\n",
      "Epoch: 460, Loss: 0.551029\n",
      "Epoch: 470, Loss: 0.563052\n",
      "Epoch: 480, Loss: 0.567557\n",
      "Epoch: 490, Loss: 0.548439\n",
      "Epoch: 500, Loss: 0.566955\n",
      "Epoch: 510, Loss: 0.553150\n",
      "Epoch: 520, Loss: 0.546099\n",
      "Epoch: 530, Loss: 0.540423\n",
      "Epoch: 540, Loss: 0.548705\n",
      "Epoch: 550, Loss: 0.570850\n",
      "Epoch: 560, Loss: 0.545988\n",
      "Epoch: 570, Loss: 0.565127\n",
      "Epoch: 580, Loss: 0.556502\n",
      "Epoch: 590, Loss: 0.509149\n",
      "Epoch: 600, Loss: 0.517531\n",
      "Epoch: 610, Loss: 0.537262\n",
      "Epoch: 620, Loss: 0.535354\n",
      "Epoch: 630, Loss: 0.529725\n",
      "Epoch: 640, Loss: 0.561315\n",
      "Epoch: 650, Loss: 0.537408\n",
      "Epoch: 660, Loss: 0.511706\n",
      "Epoch: 670, Loss: 0.521171\n",
      "Epoch: 680, Loss: 0.530288\n",
      "Epoch: 690, Loss: 0.541703\n",
      "Epoch: 700, Loss: 0.526604\n",
      "Epoch: 710, Loss: 0.559541\n",
      "Epoch: 720, Loss: 0.511271\n",
      "Epoch: 730, Loss: 0.513041\n",
      "Epoch: 740, Loss: 0.524357\n",
      "Epoch: 750, Loss: 0.501410\n",
      "Epoch: 760, Loss: 0.518702\n",
      "Epoch: 770, Loss: 0.513884\n",
      "Epoch: 780, Loss: 0.487136\n",
      "Epoch: 790, Loss: 0.486689\n",
      "Epoch: 800, Loss: 0.521604\n",
      "Epoch: 810, Loss: 0.520819\n",
      "Epoch: 820, Loss: 0.510357\n",
      "Epoch: 830, Loss: 0.513772\n",
      "Epoch: 840, Loss: 0.525759\n",
      "Epoch: 850, Loss: 0.529240\n",
      "Epoch: 860, Loss: 0.497459\n",
      "Epoch: 870, Loss: 0.503042\n",
      "Epoch: 880, Loss: 0.504135\n",
      "Epoch: 890, Loss: 0.502035\n",
      "Epoch: 900, Loss: 0.536605\n",
      "Epoch: 910, Loss: 0.510100\n",
      "Epoch: 920, Loss: 0.542357\n",
      "Epoch: 930, Loss: 0.524063\n",
      "Epoch: 940, Loss: 0.487961\n",
      "Epoch: 950, Loss: 0.470962\n",
      "Epoch: 960, Loss: 0.502845\n",
      "Epoch: 970, Loss: 0.493754\n",
      "Epoch: 980, Loss: 0.576145\n",
      "Epoch: 990, Loss: 0.509483\n",
      "Epoch: 1000, Loss: 0.486801\n",
      "Epoch: 1010, Loss: 0.500567\n",
      "Epoch: 1020, Loss: 0.484155\n",
      "Epoch: 1030, Loss: 0.478183\n",
      "Epoch: 1040, Loss: 0.480316\n",
      "Epoch: 1050, Loss: 0.525882\n",
      "Epoch: 1060, Loss: 0.480414\n",
      "Epoch: 1070, Loss: 0.474457\n",
      "Epoch: 1080, Loss: 0.462641\n",
      "Epoch: 1090, Loss: 0.482421\n",
      "Epoch: 1100, Loss: 0.471779\n",
      "Epoch: 1110, Loss: 0.485793\n",
      "Epoch: 1120, Loss: 0.464028\n",
      "Epoch: 1130, Loss: 0.484674\n",
      "Epoch: 1140, Loss: 0.509446\n",
      "Epoch: 1150, Loss: 0.466966\n",
      "Epoch: 1160, Loss: 0.502006\n",
      "Epoch: 1170, Loss: 0.477784\n",
      "Epoch: 1180, Loss: 0.485000\n",
      "Epoch: 1190, Loss: 0.487397\n",
      "Epoch: 1200, Loss: 0.483006\n",
      "Epoch: 1210, Loss: 0.471675\n",
      "Epoch: 1220, Loss: 0.450383\n",
      "Epoch: 1230, Loss: 0.479375\n",
      "Epoch: 1240, Loss: 0.495306\n",
      "Epoch: 1250, Loss: 0.480250\n",
      "Epoch: 1260, Loss: 0.476100\n",
      "Epoch: 1270, Loss: 0.483933\n",
      "Epoch: 1280, Loss: 0.512771\n",
      "Epoch: 1290, Loss: 0.514571\n",
      "Epoch: 1300, Loss: 0.510361\n",
      "Epoch: 1310, Loss: 0.529128\n",
      "Epoch: 1320, Loss: 0.483725\n",
      "Epoch: 1330, Loss: 0.505111\n",
      "Epoch: 1340, Loss: 0.514933\n",
      "Epoch: 1350, Loss: 0.465359\n",
      "Epoch: 1360, Loss: 0.451543\n",
      "Epoch: 1370, Loss: 0.446043\n",
      "Epoch: 1380, Loss: 0.458552\n",
      "Epoch: 1390, Loss: 0.477619\n",
      "Epoch: 1400, Loss: 0.550885\n",
      "Epoch: 1410, Loss: 0.444236\n",
      "Epoch: 1420, Loss: 0.448275\n",
      "Epoch: 1430, Loss: 0.444076\n",
      "Epoch: 1440, Loss: 0.488626\n",
      "Epoch: 1450, Loss: 0.475898\n",
      "Epoch: 1460, Loss: 0.450707\n",
      "Epoch: 1470, Loss: 0.485402\n",
      "Epoch: 1480, Loss: 0.461119\n",
      "Epoch: 1490, Loss: 0.459487\n",
      "Epoch: 1500, Loss: 0.511577\n",
      "time: 53.5 s (started: 2023-12-15 10:54:07 -05:00)\n"
     ]
    }
   ],
   "source": [
    "learning_rate = 1\n",
    "\n",
    "optimizer = optim.SGD(model_fnn.parameters(), lr=learning_rate)\n",
    "\n",
    "loss_fn = nn.BCEWithLogitsLoss()\n",
    "\n",
    "n_epochs = 1501\n",
    "\n",
    "#This is the Training\n",
    "\n",
    "for epoch in range(n_epochs):\n",
    "    outputs = model_fnn(X_train_PCA_t)\n",
    "    loss = loss_fn(outputs, Y_train_t)\n",
    "\n",
    "    optimizer.zero_grad()\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "    if epoch % 10 == 0:  # Print loss every 10 epochs\n",
    "      print(\"Epoch: %d, Loss: %f\" % (epoch, float(loss)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Accuracy: 0.458678\n",
      "Training Precision: 0.229339\n",
      "Training Recall: 0.500000\n",
      "Training F1 score: 0.314448\n",
      "time: 15 ms (started: 2023-12-15 10:55:00 -05:00)\n"
     ]
    }
   ],
   "source": [
    "# Checking the Accuracy, Recall, Precision, and F1 Score of the Training Set\n",
    "\n",
    "correct_train = 0\n",
    "total_train = 0\n",
    "\n",
    "with torch.no_grad():\n",
    "    outputs_train = model_fnn(X_train_PCA_t)\n",
    "    _, predicted_train = torch.max(outputs_train, dim=1)\n",
    "    total_train += Y_train_t.size(0)\n",
    "    correct_train += int((predicted_train == Y_train_t.squeeze()).sum())\n",
    "\n",
    "accuracy_train = correct_train / total_train\n",
    "precision_train = precision_score(Y_train_t, predicted_train, average='macro', zero_division=0)\n",
    "recall_train = recall_score(Y_train_t, predicted_train, average='macro', zero_division=0)\n",
    "F1_score_train = (2*recall_train*precision_train)/(recall_train+precision_train)\n",
    "\n",
    "# Print the metrics\n",
    "print(\"Training Accuracy: %f\" % accuracy_train)\n",
    "print(\"Training Precision: %f\" % precision_train)\n",
    "print(\"Training Recall: %f\" % recall_train)\n",
    "print(\"Training F1 score: %f\" % F1_score_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Accuracy: 0.442623\n",
      "Training Precision: 0.221311\n",
      "Training Recall: 0.500000\n",
      "Training F1 score: 0.306818\n",
      "time: 16 ms (started: 2023-12-15 10:55:00 -05:00)\n"
     ]
    }
   ],
   "source": [
    "# Checking the Accuracy, Recall, Precision, and F1 Score of the Validation Set\n",
    "\n",
    "correct_val = 0\n",
    "total_val = 0\n",
    "\n",
    "with torch.no_grad():\n",
    "    outputs_val = model_fnn(X_val_PCA_t)\n",
    "    _, predicted_val = torch.max(outputs_val, dim=1)\n",
    "    total_val += Y_val_t.size(0)\n",
    "    correct_val += int((predicted_val == Y_val_t.squeeze()).sum())\n",
    "\n",
    "accuracy_val = correct_val / total_val\n",
    "precision_val = precision_score(Y_val_t, predicted_val, average='macro', zero_division=0)\n",
    "recall_val = recall_score(Y_val_t, predicted_val, average='macro', zero_division=0)\n",
    "F1_score_val = (2*recall_val*precision_val)/(recall_val+precision_val)\n",
    "\n",
    "# Print the metrics\n",
    "print(\"Validation Accuracy: %f\" % accuracy_val)\n",
    "print(\"Training Precision: %f\" % precision_val)\n",
    "print(\"Training Recall: %f\" % recall_val)\n",
    "print(\"Training F1 score: %f\" % F1_score_val)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
